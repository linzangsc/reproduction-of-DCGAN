# 生成式对抗网络GAN

## 写在前面

此前介绍的三种生成模型：自回归、隐变量和流模型，优化目标都是最大化对数似然MLE（无论是直接的还是近似的优化），但这实际上是隐含着一个假设的，那就是我们认为，最大化对数似然和提升采样图像的质量是直接正相关的，即最大化对数似然就等于提升采样图像的质量。但在接下来的例子我们会看到，具有很大的对数似然的生成模型并不总是等同于这个模型具有很高的采样质量（sample quality）。

结论：对数似然并不是生成模型的gold metric，对数似然的优化并不总是和采样质量划等号。

这是GAN的motivation，探索MLE之外的优化目标，直接以采样质量（sample quality）为目标。由于GAN模型没法显示地写出似然函数，因此GAN属于一种implicit model。

## 生成器

GAN生成样本的部分称为生成器，我们用$$G$$表示，它接收一个简单分布$$p(z)$$中采样一个随机变量$$z$$为输入，通过神经网络映射到样本$$x$$。由于GAN并不以MLE为目标，它对生成器并没有特别的要求，生成器可以是由神经网络构建的非常复杂的非线性映射，这种做法至少在以下两方面解放了模型：

1. 不需要像VAE那样，输出一个分布并从分布中进行采样和加噪获得样本；
2. 不需要像流模型那样，对映射函数有较严格的要求（可逆、jacobian好算、输入输出同维）从而限制了模型的表达能力；

因此，GAN的生成器具有非常强的表达能力。虽然我们没法显式地写出生成器生成的样本的分布$$p_{\theta}(x)$$，因为生成器输出的直接是sample $$x$$且我们没有像此前梳理的生成模型那样预先定义$$p_{\theta}$$的形式，但我们知道这个分布是存在的，那么为了提高模型的采样质量，我们希望模型生成的样本的分布$$p_{\theta}(x)$$和$$p_{data}(x)$$的距离越小越好。但是我们现在写不出$$p_{\theta}(x)$$，只有$$x \sim p_{\theta}(x)$$以及$$x \sim p_{data}(x)$$，有什么其它方法可以实现这个目标？GAN采用一个判别器来实现这个目标。

## 判别器

判别器实际上是一个神经网络搭建的二分类器，我们用$$D$$表示，它接收$$x$$为输入，输出一个概率，这个概率的含义是$$x$$来自$$p_{data}(x)$$的概率，即$$x$$是真实数据集中的样本的概率。我们的数据天生地带有标签，训练集中的$$x$$带有“来自真实分布”的标签，生成器生成的$$x$$带有“不来自真实分布”的标签，因此判别器可以对$$x$$进行二分类。

对于目标是二分类的判别器来说，以“来自真实分布”为标签1，它的优化目标是两种交叉熵之和：

$$\begin{align}
\max \limits_{D} \mathbb{E}_{x \sim p_{data}(x)}[\log D(x)] + \mathbb{E}_{x \sim p_{\theta}(x)} [\log (1-D(x)] \tag{1} \\
\end{align}$$

式（1）的第一项是来自真实分布的样本$$x$$对应的交叉熵，标签为1，第二项是生成器生成的样本对应的交叉熵，标签为0。对于判别器来说，它希望它的输出概率在输入为真实样本时接近1，在输入为生成的样本时接近0，因此它希望最大化式（1）。

式（1）所优化的参数是判别器$$D$$的参数，而对于生成器$$G$$而言，它的目标和判别器是相反的，生成器希望的是即使是一个训练有素的判别器，对生成器输出的样本的输出概率也应该接近1，即判别器无法区分生成样本和来自真实分布的样本。可以看到式（1）中生成器只出现在了第二项，因此生成器的优化目标实际上是：

$$\begin{align}
\min \limits_{G}  \mathbb{E}_{x \sim p_{\theta}(x)} [\log (1-D(x))] \tag{2} \\
\end{align}$$

把生成器和判别器的优化目标写在一起，得到GAN的优化目标：

$$\begin{align}
\min \limits_{G}  \max \limits_{D} \mathbb{E}_{x \sim p_{data}(x)}[\log D(x)] +  \mathbb{E}_{x \sim p_{\theta}(x)} [\log (1-D(x))] \tag{3} \\
\end{align}$$

## 从JSD散度的角度理解GAN的优化目标

可以看到优化目标确实并不要求显式地写出模型分布$$p_{\theta}(x)$$，只用到了sample来计算，但是问题在于，式（3）能够实现我们前面提到的最小化$$p_{\theta}(x)$$和$$p_{data}(x)$$之间距离的目标吗？答案是可以的。前面提到，我们要用判别器来对生成样本做分类，因此在给定生成样本的分布也就是固定了生成器$$G$$的情况下，我们先来看内层对判别器$$D$$的优化目标式（1），将期望展开：

$$\begin{align}
\mathcal{L} =  \int p_{data}(x)\log D(x) + p_{\theta}(x) \log (1-D(x)) dx \tag{4} \\
\end{align}$$

对积分求最大值可以转化为求被积函数的最大值，设被积函数为$$f(D(x))$$，我们对被积函数求导并令导数为0，得到：

$$\begin{align}
\frac{\partial f(D(x))}{\partial D(x)} &= \frac{\partial p_{data}(x)\log D(x) + p_{\theta}(x) \log (1-D(x))}{\partial D(x)}\tag{5}  \\
&= \frac{p_{data}(x)}{D(x)} - \frac{p_{\theta}(x)}{1-D(x)} = 0 \tag{6} \\
\end{align}$$

$$p_{data}(x)$$和$$p_{\theta}(x)$$在给定$$x$$的情况下是标量，因此我们可以求解得到在给定$$G$$的情况下，优化目标取得最优的判别器$$D^*(x) = \frac{p_{data}(x)}{p_{data}(x) + p_{\theta}(x)}$$。理想情况下，我们得到了最优的判别器，并将其输出代回式（3）进行生成器$$G$$的优化：

$$\begin{align}
\min \limits_{G}  & \mathbb{E}_{x \sim p_{data}(x)}[\log \frac{p_{data}(x)}{p_{data}(x) + p_{\theta}(x)}] +  \mathbb{E}_{x \sim p_{\theta}(x)} [\log (1-\frac{p_{data}(x)}{p_{data}(x) + p_{\theta}(x)})] \tag{7} \\
=& \mathbb{E}_{x \sim p_{data}(x)}[\log \frac{p_{data}(x)}{\frac{p_{data}(x) + p_{\theta}(x)}{2}}] +  \mathbb{E}_{x \sim p_{\theta}(x)} [\log (\frac{p_{\theta}(x)}{\frac{p_{data}(x) + p_{\theta}(x)}{2}})] -\log4 \tag{8} \\
=& D_{KL}(p_{data}(x)||\frac{p_{data}(x) + p_{\theta}(x)}{2}) + D_{KL}( p_{\theta}(x)||\frac{p_{data}(x) + p_{\theta}(x)}{2})-\log4 \tag{9} \\
=& 2D_{JSD}(p_{data}(x)||p_{\theta}(x)) - \log4 \tag{10} \\
\end{align}$$

式（10）表明，在得到当前$$G$$对应的最优的$$D$$的情况下，对$$G$$的优化实际上是在最小化模型分布和真实分布之间的JSD散度（琴生香农散度），也就说明，对式（3）的优化确实是如我们预期的在最小化模型分布和真实分布之间的距离。
