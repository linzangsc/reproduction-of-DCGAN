# 生成式对抗网络GAN

## 写在前面

此前介绍的三种生成模型：自回归、隐变量和流模型，优化目标都是最大化对数似然MLE（无论是直接的还是近似的优化），但这实际上是隐含着一个假设的，那就是我们认为，最大化对数似然和提升采样图像的质量是直接正相关的，即最大化对数似然就等于提升采样图像的质量。但在接下来的例子我们会看到，具有很大的对数似然的生成模型并不总是等同于这个模型具有很高的采样质量（sample quality）。

结论：对数似然并不是生成模型的gold metric，对数似然的优化并不总是和采样质量划等号。

这是GAN的motivation，探索MLE之外的优化目标，直接以采样质量（sample quality）为目标。由于GAN模型没法显示地写出似然函数，因此GAN属于一种implicit model。

## 生成器

GAN生成样本的部分称为生成器，我们用$$G$$表示，它接收一个简单分布$$p(z)$$中采样一个随机变量$$z$$为输入，通过神经网络映射到样本$$x$$。由于GAN并不以MLE为目标，它对生成器并没有特别的要求，生成器可以是由神经网络构建的非常复杂的非线性映射，这种做法至少在以下两方面解放了模型：

1. 不需要像VAE那样，输出一个分布并从分布中进行采样和加噪获得样本；
2. 不需要像流模型那样，对映射函数有较严格的要求（可逆、jacobian好算、输入输出同维）从而限制了模型的表达能力；

因此，GAN的生成器具有非常强的表达能力。虽然我们没法显式地写出生成器生成的样本的分布$$p_{\theta}(x)$$，因为生成器输出的直接是sample $$x$$且我们没有像此前梳理的生成模型那样预先定义$$p_{\theta}$$的形式，但我们知道这个分布是存在的，那么为了提高模型的采样质量，我们希望模型生成的样本的分布$$p_{\theta}(x)$$和$$p_{data}(x)$$的距离越小越好。但是我们现在写不出$$p_{\theta}(x)$$，只有$$x \sim p_{\theta}(x)$$以及$$x \sim p_{data}(x)$$，有什么其它方法可以实现这个目标？GAN采用一个判别器来实现这个目标。

## 判别器

判别器实际上是一个神经网络搭建的二分类器，我们用$$D$$表示，它接收$$x$$为输入，输出一个概率，这个概率的含义是$$x$$来自$$p_{data}(x)$$的概率，即$$x$$是真实数据集中的样本的概率。我们的数据天生地带有标签，训练集中的$$x$$带有“来自真实分布”的标签，生成器生成的$$x$$带有“不来自真实分布”的标签，因此判别器可以对$$x$$进行二分类。

对于目标是二分类的判别器来说，以“来自真实分布”为标签1，它的优化目标是两种交叉熵之和：

$$\begin{align}
\max \limits_{D} \mathbb{E}_{x \sim p_{data}(x)}[\log D(x)] + \mathbb{E}_{x \sim p_{\theta}(x)} [\log (1-D(x)] \tag{1} \\
\end{align}$$

式（1）的第一项是来自真实分布的样本$$x$$对应的交叉熵，标签为1，第二项是生成器生成的样本对应的交叉熵，标签为0。对于判别器来说，它希望它的输出概率在输入为真实样本时接近1，在输入为生成的样本时接近0，因此它希望最大化式（1）。

式（1）所优化的参数是判别器$$D$$的参数，而对于生成器$$G$$而言，它的目标和判别器是相反的，生成器希望的是即使是一个训练有素的判别器，对生成器输出的样本的输出概率也应该接近1，即判别器无法区分生成样本和来自真实分布的样本。可以看到式（1）中生成器只出现在了第二项，因此生成器的优化目标实际上是：

$$\begin{align}
\min \limits_{G}  \mathbb{E}_{x \sim p_{\theta}(x)} [\log (1-D(x))] \tag{2} \\
\end{align}$$

把生成器和判别器的优化目标写在一起，得到GAN的优化目标：

$$\begin{align}
\min \limits_{G}  \max \limits_{D} \mathbb{E}_{x \sim p_{data}(x)}[\log D(x)] +  \mathbb{E}_{x \sim p_{\theta}(x)} [\log (1-D(x))] \tag{3} \\
\end{align}$$

## 从JSD散度的角度理解GAN的优化目标

可以看到优化目标确实并不要求显式地写出模型分布$$p_{\theta}(x)$$，只用到了sample来计算，但是问题在于，式（3）能够实现我们前面提到的最小化$$p_{\theta}(x)$$和$$p_{data}(x)$$之间距离的目标吗？答案是可以的。前面提到，我们要用判别器来对生成样本做分类，因此在给定生成样本的分布也就是固定了生成器$$G$$的情况下，我们先来看内层对判别器$$D$$的优化目标式（1），将期望展开：

$$\begin{align}
\mathcal{L} =  \int p_{data}(x)\log D(x) + p_{\theta}(x) \log (1-D(x)) dx \tag{4} \\
\end{align}$$

对积分求最大值可以转化为求被积函数的最大值，设被积函数为$$f(D(x))$$，我们对被积函数求导并令导数为0，得到：

$$\begin{align}
\frac{\partial f(D(x))}{\partial D(x)} &= \frac{\partial p_{data}(x)\log D(x) + p_{\theta}(x) \log (1-D(x))}{\partial D(x)}\tag{5}  \\
&= \frac{p_{data}(x)}{D(x)} - \frac{p_{\theta}(x)}{1-D(x)} = 0 \tag{6} \\
\end{align}$$

$$p_{data}(x)$$和$$p_{\theta}(x)$$在给定$$x$$的情况下是标量，因此我们可以求解得到在给定$$G$$的情况下，优化目标取得最优的判别器$$D^*(x) = \frac{p_{data}(x)}{p_{data}(x) + p_{\theta}(x)}$$。理想情况下，我们得到了最优的判别器，并将其输出代回式（3）进行生成器$$G$$的优化：

$$\begin{align}
\min \limits_{G}  & \mathbb{E}_{x \sim p_{data}(x)}[\log \frac{p_{data}(x)}{p_{data}(x) + p_{\theta}(x)}] +  \mathbb{E}_{x \sim p_{\theta}(x)} [\log (1-\frac{p_{data}(x)}{p_{data}(x) + p_{\theta}(x)})] \tag{7} \\
=& \mathbb{E}_{x \sim p_{data}(x)}[\log \frac{p_{data}(x)}{\frac{p_{data}(x) + p_{\theta}(x)}{2}}] +  \mathbb{E}_{x \sim p_{\theta}(x)} [\log (\frac{p_{\theta}(x)}{\frac{p_{data}(x) + p_{\theta}(x)}{2}})] -\log4 \tag{8} \\
=& D_{KL}(p_{data}(x)||\frac{p_{data}(x) + p_{\theta}(x)}{2}) + D_{KL}( p_{\theta}(x)||\frac{p_{data}(x) + p_{\theta}(x)}{2})-\log4 \tag{9} \\
=& 2D_{JSD}(p_{data}(x)||p_{\theta}(x)) - \log4 \tag{10} \\
\end{align}$$

式（10）表明，在得到当前$$G$$对应的最优的$$D$$的情况下，对$$G$$的优化实际上是在最小化模型分布和真实分布之间的JSD散度（琴生香农散度），也就说明，对式（3）的优化确实是如我们预期的在最小化模型分布和真实分布之间的距离。这是一个很奇妙的结论，这表明，即使我们没法显式地写出似然，但我们依然可以通过引入判别器的方式引入似然比来达到优化散度的目的。由于散度的非负性，式（10）的最小值为$$-\log4$$。

## 问题

注意我们推导JSD散度的形式时，是要求内层对$$D$$的优化已经取得最优，然后再对$$G$$进行优化，因此在训练GAN时，对$$D$$和$$G$$的优化是交替的。然而实际情况下$$D$$并不会通过短短几次梯度下降就收敛到最优，而用一个非最优的$$D$$计算得到的对$$G$$的梯度，这个梯度是不可解释的，我们不知道它会把$$G$$往什么方向优化。另一方面，GAN的优化目标是adversarial的，是一个minimax问题，即两个神经网络的优化方向是相反的，而像VAE的encoder和decoder，它们的优化方向也是一致的。以上提到的两点原因会给GAN的训练带来困难，容易出现loss震荡甚至突然爆炸这种现象。

另一个GAN常见的问题就是mode collapse模式坍塌，即生成器到最后只会生成几张它见过的来自训练集的图像，直觉上来说这是生成器走了捷径，每次都只生成几张在$$D$$上得分最高的图像来欺骗$$D$$。mode collapse还可能和JSD散度的性质有关，JSD散度的性质决定了它可能会存在多个局部最优解也就是捷径，一旦模型陷入这些局部最优，就很难再跳出来。例如，我们用$$G$$去拟合一个两个峰分别是-3和3的GMM，把JSD的等高线可视化出来，可以看到JSD除了(-3, 3)和(3, -3)两个全局最优以外，还存在(3, 3)和(-3, -3)两个局部最优，一旦陷入这两个局部最优，意味着模型只学到了一个峰，实际上就是mode collapse。

为了解决上述的JSD散度性质的问题，很多工作尝试了换散度，但事后看来换散度并不能解决GAN难收敛和mode collapse的问题。最终发现能够较有效地解决这些问题的思路主要集中在网络架构的设计上。从DCGAN到PGAN再到StyleGAN，网络架构的优化带来了GAN的生成质量的飞跃。
